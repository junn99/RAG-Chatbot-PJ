# 1ì˜ ì˜¤ë¥˜í•´ê²° ë²„ì „
import streamlit as st
from langchain_core.messages import HumanMessage, AIMessage
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from langchain import hub
import os

# Constants
PDF_PATH = "/home/jun/my_project/jun/teddy/.cache/files/(IS-168) ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì— ëŒ€í•œ ì¤‘ì†Œê¸°ì—…ì˜ ì¸ì‹ ë° ìˆ˜ìš” ì¡°ì‚¬ë¶„ì„.pdf"  # Update this with your PDF file path
VECTOR_STORE_PATH = ".cache/embeddings/vectorstore.pkl"
CACHE_DIR = ".cache"
EMBEDDINGS_DIR = ".cache/embeddings"

# Create cache directories
os.makedirs(CACHE_DIR, exist_ok=True)
os.makedirs(EMBEDDINGS_DIR, exist_ok=True)

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "chain" not in st.session_state:
    st.session_state.chain = None

# Function to get embeddings
@st.cache_resource
def get_embeddings():
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-m3",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )

# Function to load or create vector store
@st.cache_resource(show_spinner="ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ... or ìƒì„±...ì¤‘ì…ë‹ˆë‹¤.")
def load_or_create_vector_store():
    embeddings = get_embeddings()
    
    if os.path.exists(VECTOR_STORE_PATH):
        st.warning("ê¸°ì¡´ì˜ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¡œë“œì¤‘ì…ë‹ˆë‹¤. ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” íŒŒì¼ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return FAISS.load_local(VECTOR_STORE_PATH, embeddings, allow_dangerous_deserialization=True)
    
    loader = PDFPlumberLoader(PDF_PATH)
    docs = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
    split_documents = text_splitter.split_documents(docs)
    
    vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)
    vectorstore.save_local(VECTOR_STORE_PATH)
    return vectorstore

# Function to create RAG chain
@st.cache_resource(show_spinner="RAGì²´ì¸ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..")
def create_rag_chain():
    vectorstore = load_or_create_vector_store()
    retriever = vectorstore.as_retriever()
    
    # prompt = hub.pull("rlm/rag-prompt")
    prompt = ChatPromptTemplate.from_template("""
ì£¼ì–´ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ì£¼ì„¸ìš”:

1. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
2. ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
3. í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ê³  ëª¨ë¥¸ë‹¤ê³  í•˜ì„¸ìš”.
4. ë‹µë³€ì€ 3-4ë¬¸ì¥ì„ ë„˜ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:
""")
    
    llm = ChatOllama(model="gemma2", temperature=0)
    
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# Streamlit UI
st.title("PDF-based Q&A ğŸ’¬")

# Initialize RAG chain
if st.session_state.chain is None:
    st.session_state.chain = create_rag_chain()

# Display chat history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# User input
if prompt := st.chat_input("ë“œë£¨ì™€"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # Stream the response
        for chunk in st.session_state.chain.stream(prompt):
            full_response += chunk
            message_placeholder.markdown(full_response + "â–Œ")
        
        message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# Add a button to clear the chat history
if st.button("Clear Chat History"):
    st.session_state.messages = []
    st.rerun()